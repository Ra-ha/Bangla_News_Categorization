from google.colab import drive
drive.mount('/content/drive')
from google.colab import drive
drive.mount('/root/gdrive', force_remount=True)
%cd /root/gdrive/MyDrive/Bangla Newspaper classification

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import pickle
from collections import Counter

import json

with open('data.json', encoding='utf-8') as fh:
    orig_data = json.load(fh)
print(orig_data[0])

data = orig_data[:23000]

all_categories = set([sample['category'] for sample in data])
print(all_categories)
print(len(all_categories))

category_count_list = []
count = 0
for cat in all_categories:
  count = 0
  for row in data:
    if row['category'] == cat:
      count += 1
  category_count_list.append({'category': cat, 'count': count})
print(category_count_list)

from operator import itemgetter
sorted_cat_count_list = []
# sorted_cat_count_list = sorted(category_count_list, key=lambda x: x.count, reverse=True)
category_count_list.sort(key = itemgetter('count'), reverse=True)
print(category_count_list)

selected_cats = []

for p in category_count_list:
    if p['count'] > 300:
        selected_cats.append(p['category'])
print(selected_cats)
print(len(selected_cats))

x_text = []
y_label = []

for row in data:
    if row['category'] in selected_cats:
        y_label.append(row['category'])
        x_text.append(row['content'])
print(len(x_text),len(y_label))

X_test = x_text[0:5000]
y_test = y_label[0:5000]
X_train = x_text[5000:]
y_train = y_label[5000:]
print(len(X_train),len(X_test))

import tensorflow as tf
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.initializers import TruncatedNormal
from tensorflow.keras.losses import CategoricalCrossentropy
from tensorflow.keras.metrics import CategoricalAccuracy
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.layers import Input, Dense

!pip install transformers
import transformers

!pip install sentencepiece

from transformers import AutoTokenizer, TFAutoModel, TFXLMRobertaModel

bert_tokenizer = AutoTokenizer.from_pretrained('jplu/tf-xlm-roberta-base')
# bert_model = TFAutoModel.from_pretrained("jplu/tf-xlm-roberta-large")
bert_model = TFXLMRobertaModel.from_pretrained('jplu/tf-xlm-roberta-base')


# Tokenize the input (takes some time) 
# here tokenizer using from bert-base-cased
x_train = bert_tokenizer(
    text=X_train,
    add_special_tokens=True,
    max_length=512,
    truncation=True,
    padding=True, 
    return_tensors='tf',
    return_token_type_ids = False,
    return_attention_mask = True,
    verbose = True)
x_test = bert_tokenizer(
    text=X_test,
    add_special_tokens=True,
    max_length=512,
    truncation=True,
    padding=True, 
    return_tensors='tf',
    return_token_type_ids = False,
    return_attention_mask = True,
    verbose = True)

input_ids = x_train['input_ids']
attention_mask = x_train['attention_mask']

from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder
def make_ohe(y_label):
  # Label Encode
  encoder = LabelEncoder()
  class_labels = encoder.fit_transform(y_label)
  print(class_labels,len(class_labels),class_labels.shape)
  print(set(class_labels))

  #One hot Encode
  encoder = OneHotEncoder(sparse=False)
  class_labels = class_labels.reshape((class_labels.shape[0], 1))
  y_ohe = encoder.fit_transform(class_labels)
  print(y_ohe,y_ohe.shape)
  return y_ohe

  y_train = make_ohe(y_train)
print(type(y_train))

y_test = make_ohe(y_test)
print(type(y_test))

max_len = 512
input_ids = Input(shape=(max_len,), dtype=tf.int32, name="input_ids")
input_mask = Input(shape=(max_len,), dtype=tf.int32, name="attention_mask")
embeddings = bert_model(input_ids,attention_mask = input_mask)[0] 
out = tf.keras.layers.GlobalMaxPool1D()(embeddings)
# out = Dense(128, activation='relu')(out)
# out = tf.keras.layers.Dropout(0.1)(out)
out = Dense(32,activation = 'relu')(out)
y = Dense(10,activation = 'softmax')(out)
model = tf.keras.Model(inputs=[input_ids, input_mask], outputs=y)
model.layers[2].trainable = True
model.summary()

optimizer = Adam(
    learning_rate=5e-05, # this learning rate is for bert model , taken from huggingface website 
    epsilon=1e-08,
    decay=0.01,
    clipnorm=1.0)
# Set loss and metrics
# loss =CategoricalCrossentropy(from_logits = True)
# metrics = CategoricalAccuracy('balanced_accuracy'),
loss='categorical_crossentropy'
metrics=['accuracy']
# Compile the model
model.compile(
    optimizer = optimizer,
    loss = loss, 
    metrics = metrics)


checkpoint = tf.keras.callbacks.ModelCheckpoint('xlmRoberta_22k.h5', monitor='loss', save_best_only=True, verbose=1)
earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3, verbose=1)

train_history = model.fit(x ={'input_ids':x_train['input_ids'],'attention_mask':x_train['attention_mask']},validation_split=0.2,callbacks=[checkpoint, earlystopping],y= y_train, epochs=5, batch_size = 6)

model.save("xlmRoberta_22k.h5")
model.save("xlmRoberta_22k.h5")

predicted_raw_train = model.predict({'input_ids':x_train['input_ids'],'attention_mask':x_train['attention_mask']})
predicted_raw_train[0]

y_predicted_train = np.argmax(predicted_raw_train, axis = 1)
y_true_train = np.argmax(y_train, axis = 1)
print(classification_report(y_true_train, y_predicted_train))

predicted_raw = model.predict({'input_ids':x_test['input_ids'],'attention_mask':x_test['attention_mask']})
predicted_raw[0]

y_predicted = np.argmax(predicted_raw, axis = 1)
y_true = np.argmax(y_test, axis = 1)
print(classification_report(y_true, y_predicted))
